# Halo Metrics Deployment on GKE

## Background

The configuration for the [`dev` environment](../../src/main/k8s/dev) can be
used as the basis for deploying CMMS components using Google Kubernetes Engine
(GKE) on another Google Cloud project.

Many operations can be done either via the gcloud CLI or the Google Cloud web
console. This guide picks whichever is most convenient for that operation. Feel
free to use whichever you prefer.

### What are we creating/deploying?
- 1 GKE cluster
    - 1 Managed Service for Prometheus
    - 1 cert-manager
    - 1 OpenTelemetry Operator
    - 1 GMP ClusterPodMonitoring
        - prometheus-pod-monitor
    - 1 GMP PodMonitoring
      - collector-pod-monitor
    - 1 OpenTelemetry Operator OpenTelemetryCollector
      - default-sidecar
    - 1 OpenTelemetry Operator Instrumentation
      - open-telemetry-java-agent

## Before you start

Create a cluster for a Halo component. Complete a guide up until the "create
the cluster" step. See 
[Create Kingdom Cluster](kingdom-deployment.md#step-4-create-the-cluster),
[Create Duchy Cluster](duchy-deployment.md#step-5-create-the-cluster), or
[Create Reporting Cluster](reporting-server-deployment.md#create-the-cluster).

### Quick start

Adjust the number of nodes and machine type according to your expected usage.

## Deploy Managed Service for Prometheus

Enable Managed Service for Prometheus on the cluster. It is a quick toggle on
Google Cloud web console and everything will be handled automatically.

## Install cert-manager

The cert-manager version corresponds with the OpenTelemetry Operator version.
Only the versions below have been tested.

```shell
kubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.9.1/cert-manager.yaml
```

## Install OpenTelemetry Operator

```shell
kubectl apply -f https://github.com/open-telemetry/opentelemetry-operator/releases/download/v0.60.0/opentelemetry-operator.yaml
```

## Create the K8s manifest

Deploying to the cluster is generally done by applying a K8s manifest. You can
use the `dev` configuration as a base to get started. The `dev` manifest is a
YAML file that is generated from files written in [CUE](https://cuelang.org/)
using Bazel rules.

The main files for the `dev` Metrics are
[`prometheus_gke.cue`](../../src/main/k8s/dev/prometheus_gke.cue) and
[`open_telemetry_gke.cue`](../../src/main/k8s/dev/open_telemetry_gke.cue).

You can modify the filtering of the OpenTelemetry metrics. The config is found
in [`open_telemetry.cue`](../../src/main/k8s/open_telemetry.cue)

To generate the YAML manifests from the CUE files, run the following:

```shell
bazel build //src/main/k8s/dev:prometheus_gke
bazel build //src/main/k8s/dev:open_telemetry_gke
```

You can also do your customization to the generated YAML file rather than to the
CUE file.

## Apply the K8s manifest

If you're using manifests generated by the above Bazel targets, the commands to
apply the manifests is

```shell
kubectl apply -f bazel-bin/src/main/k8s/dev/prometheus_gke.yaml
kubectl apply -f bazel-bin/src/main/k8s/dev/open_telemetry_gke.yaml
```

Substitute the paths if you're using different K8s manifests.

Now all components should be successfully deployed to your GKE cluster. You can
verify by running

```shell
kubectl get clusterpodmonitorings
```

```shell
kubectl get -n gmp-system podmonitorings
```

```shell
kubectl get opentelemetrycollectors
```

```shell
kubectl get instrumentations
```


You should see something like the following:

```
NAME                     AGE
prometheus-pod-monitor   23s
```

```
NAME                    AGE
collector-pod-monitor   5m12s
```

```
NAME              MODE      VERSION   AGE
default-sidecar   sidecar   0.60.0    41s
```

```
NAME                        AGE   ENDPOINT   SAMPLER   SAMPLER ARG
open-telemetry-java-agent   68s  
```

## Apply K8s namespace annotations

This ensures every pod can have metrics.

```shell
kubectl annotate namespaces default 'sidecar.opentelemetry.io/inject'=default-sidecar
kubectl annotate namespaces default 'instrumentation.opentelemetry.io/inject-java'=true
```

You can verify by running

```shell
kubectl describe namespace default
```

You should see something like the following:

```
Name:         default
Labels:       kubernetes.io/metadata.name=default
Annotations:  instrumentation.opentelemetry.io/inject-java: true
              sidecar.opentelemetry.io/inject: default-sidecar
Status:       Active
```

## Restart Deployments to Start Collecting Metrics

If deployments were started before these steps, they have to be restarted.
Verify the pods have the label `scrape=true` with

```shell
kubectl describe pod <NAME-OF-POD>
```

If the label is missing, recreate the k8s manifest from the latest cue files
that add the label then apply the updated manifests before restarting.

## Verify Managed Prometheus can Scrape Metrics

Visit the [Managed Prometheus](https://console.cloud.google.com/monitoring/prometheus) page
in Cloud Console. Query `up` and `scrape_samples_scraped`. 

The first one tells you which targets it can find and whether they are up, and 
the latter is a good way to check that scraping is occurring. If it 
hasn't been long enough, the latter might show all 0's, but after a couple of
minutes you should be seeing results for every target that is up.

## Adding Additional Metrics

The above adds OpenTelemetry jvm and rpc metrics. With the above as a base, it
is possible to add other metrics that can be scraped.

### kubelet and cAdvisor

See [kubelet](https://cloud.google.com/stackdriver/docs/managed-prometheus/setup-managed#kubelet-metrics)

